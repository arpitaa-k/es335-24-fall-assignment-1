{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (126, 500, 3)\n",
      "Testing data shape:  (54, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API key from the environment variables\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Available Models\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "model = groq_models[\"llama3-70b\"]\n",
    "\n",
    "# Constants\n",
    "green = \"\\x1b[38;2;152;251;152m\"\n",
    "red = \"\\x1b[38;2;255;0;0m\"\n",
    "reset = \"\\x1b[0m\"\n",
    "time = 10\n",
    "offset = 100\n",
    "folders = [\"LAYING\", \"SITTING\", \"STANDING\", \"WALKING\", \"WALKING_DOWNSTAIRS\", \"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\": 1, \"WALKING_UPSTAIRS\": 2, \"WALKING_DOWNSTAIRS\": 3, \"SITTING\": 4, \"STANDING\": 5, \"LAYING\": 6}\n",
    "\n",
    "combined_dir = os.path.join(\"C:\\\\Users\\\\arpit\\\\OneDrive\\\\Desktop\\\\es335-24-fall-assignment-1\\\\UCI HAR Dataset\\\\Combined\")\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "dataset_dir = os.path.join(combined_dir, \"Train\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_train.append(df.values)\n",
    "        y_train.append(classes[folder])\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "dataset_dir = os.path.join(combined_dir, \"Test\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_test.append(df.values)\n",
    "        y_test.append(classes[folder])\n",
    "\n",
    "# Concatenate the training and testing data\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "\n",
    "# Split the data into training and testing sets. Change the seed value to obtain different random splits.\n",
    "seed = 4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "\n",
    "print(\"Training data shape: \", X_train.shape)\n",
    "print(\"Testing data shape: \", X_test.shape)\n",
    "\n",
    "categories = list(classes.keys())\n",
    "\n",
    "# Train prompt template\n",
    "train_prompt = lambda data, category: f\"\"\"\n",
    "You do not need to output anything for this query.\n",
    "Example data for \"{category}\":\n",
    "{data}\n",
    "\"\"\"\n",
    "\n",
    "# Test prompt template\n",
    "test_prompt = lambda data: f\"\"\"\n",
    "You will be provided 3-axis accelerometer data from one of the following activities:\n",
    "    1) WALKING\n",
    "    2) LAYING\n",
    "    3) WALKING_UPSTAIRS\n",
    "    4) WALKING_DOWNSTAIRS\n",
    "    5) SITTING\n",
    "    6) STANDING\n",
    "You have to output only the name of the predicted activity and nothing else.\n",
    "DATA:\n",
    "{data}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "1) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "2) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "3) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "4) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "5) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "6) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "7) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "8) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "9) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "10) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "11) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "12) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "13) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "14) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "15) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "16) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "17) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "18) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "19) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "20) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "21) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "22) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "23) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "24) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "25) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "26) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "27) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "28) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "29) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "30) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "31) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "32) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "33) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "34) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "35) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "36) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "37) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "38) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "39) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "40) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "41) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "42) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "43) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "44) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "45) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "46) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "47) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "48) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "49) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "50) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "51) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "52) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "53) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "\n",
      "Total: 54\n",
      "Correct: 9\n",
      "Accuracy: 0.16666666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(model=model, api_key=GROQ_API_KEY, temperature=0)\n",
    "\n",
    "correct_count = 0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    query = test_prompt(X_test[i])\n",
    "    ans = llm.invoke(query).content\n",
    "    if(ans==categories[y_test[i]-1]):\n",
    "        correct_count+=1\n",
    "        color = green\n",
    "    else:\n",
    "        color = red\n",
    "    print(f\"{i}) Actual: {categories[y_test[i]-1]}\\n{color}Predicted: {ans}{reset}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Total: {len(X_test)}\n",
    "Correct: {correct_count}\n",
    "Accuracy: {correct_count/len(X_test)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ready to learn from the provided examples. Please go ahead and provide the 3-axis accelerometer data for each of the activities (WALKING, LAYING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING). I'll learn from these examples and prepare to be tested against the test dataset.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(model=model, api_key=GROQ_API_KEY, temperature=0)\n",
    "\n",
    "print(llm.invoke(\"\"\"\n",
    "You will be provided examples for 3-axis accelerometer data from one of the following activities:\n",
    "    1) WALKING\n",
    "    2) LAYING\n",
    "    3) WALKING_UPSTAIRS\n",
    "    4) WALKING_DOWNSTAIRS\n",
    "    5) SITTING\n",
    "    6) STANDING\n",
    "You have to learn from these examples and then you will be tested against test dataset.\n",
    "You do not need to output anything for this query.\n",
    "\"\"\").content)\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    query = train_prompt(X_train[i],categories[y_train[i]-1])\n",
    "    response = llm.invoke(query)\n",
    "    #print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "1) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "2) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "3) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "4) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "5) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "6) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "7) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "8) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "9) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "10) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "11) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "12) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "13) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "14) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "15) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "16) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "17) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "18) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "19) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "20) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "21) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "22) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "23) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "24) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "25) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "26) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "27) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "28) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "29) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "30) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "31) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "32) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "33) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "34) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "35) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "36) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "37) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "38) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "39) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "40) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "41) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "42) Actual: WALKING\n",
      "\u001b[38;2;152;251;152mPredicted: WALKING\u001b[0m\n",
      "43) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "44) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "45) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "46) Actual: STANDING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "47) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "48) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "49) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "50) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING_UPSTAIRS\u001b[0m\n",
      "51) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "52) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "53) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: WALKING\u001b[0m\n",
      "Total: 54\n",
      "Correct: 9\n",
      "Accuracy: 0.16666666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    query = test_prompt(X_test[i])\n",
    "    ans = llm.invoke(query).content\n",
    "    if(ans==categories[y_test[i]-1]):\n",
    "        correct_count+=1\n",
    "        color = green\n",
    "    else:\n",
    "        color = red\n",
    "    print(f\"{i}) Actual: {categories[y_test[i]-1]}\\n{color}Predicted: {ans}{reset}\")\n",
    "\n",
    "print(f\"\"\"Total: {len(X_test)}\n",
    "Correct: {correct_count}\n",
    "Accuracy: {correct_count/len(X_test)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "1) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "2) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "3) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "4) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "5) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "6) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "7) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "8) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "9) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "10) Actual: STANDING\n",
      "\u001b[38;2;152;251;152mPredicted: STANDING\u001b[0m\n",
      "11) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "12) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "13) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "14) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "15) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "16) Actual: STANDING\n",
      "\u001b[38;2;152;251;152mPredicted: STANDING\u001b[0m\n",
      "17) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "18) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "19) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "20) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "21) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "22) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "23) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "24) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "25) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "26) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "27) Actual: STANDING\n",
      "\u001b[38;2;152;251;152mPredicted: STANDING\u001b[0m\n",
      "28) Actual: STANDING\n",
      "\u001b[38;2;152;251;152mPredicted: STANDING\u001b[0m\n",
      "29) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "30) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "31) Actual: STANDING\n",
      "\u001b[38;2;152;251;152mPredicted: STANDING\u001b[0m\n",
      "32) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "33) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "34) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "35) Actual: STANDING\n",
      "\u001b[38;2;152;251;152mPredicted: STANDING\u001b[0m\n",
      "36) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "37) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "38) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "39) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "40) Actual: STANDING\n",
      "\u001b[38;2;152;251;152mPredicted: STANDING\u001b[0m\n",
      "41) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "42) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "43) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "44) Actual: WALKING_UPSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "45) Actual: SITTING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "46) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "47) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "48) Actual: LAYING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "49) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "50) Actual: WALKING\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "51) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "52) Actual: STANDING\n",
      "\u001b[38;2;152;251;152mPredicted: STANDING\u001b[0m\n",
      "53) Actual: WALKING_DOWNSTAIRS\n",
      "\u001b[38;2;255;0;0mPredicted: STANDING\u001b[0m\n",
      "Total: 54\n",
      "Correct: 8\n",
      "Accuracy: 0.14814814814814814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_min = np.min(X_test)\n",
    "x_max = np.max(X_test)\n",
    "y_min = np.min(y_test)\n",
    "y_max = np.max(y_test)\n",
    "\n",
    "X_test = (x_max-x_min)*np.random.random(len(X_test)) + x_min\n",
    "y_test = np.random.randint(y_min,y_max+1,len(X_test))\n",
    "\n",
    "correct_count = 0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    query = test_prompt(X_test[i])\n",
    "    ans = llm.invoke(query).content\n",
    "    if(ans==categories[y_test[i]-1]):\n",
    "        correct_count+=1\n",
    "        color = green\n",
    "    else:\n",
    "        color = red\n",
    "    print(f\"{i}) Actual: {categories[y_test[i]-1]}\\n{color}Predicted: {ans}{reset}\")\n",
    "\n",
    "print(f\"\"\"Total: {len(X_test)}\n",
    "Correct: {correct_count}\n",
    "Accuracy: {correct_count/len(X_test)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatGroq(model='llama3-70b', api_key=API_KEY)\n",
    "\n",
    "# Define the activity descriptions\n",
    "activity_descriptions = {\n",
    "    \"WALKING\": \"Moving forward with alternating steps.\",\n",
    "    \"SITTING\": \"Seated with minimal movement.\",\n",
    "    \"STANDING\": \"Standing still with slight body shifts.\",\n",
    "    \"LAYING\": \"Lying down with minimal body movement.\",\n",
    "    \"WALKING_UPSTAIRS\": \"Moving upward on stairs.\",\n",
    "    \"WALKING_DOWNSTAIRS\": \"Descending stairs with alternating steps.\"\n",
    "}\n",
    "\n",
    "def create_prompt(features):\n",
    "    prompt = (\n",
    "        \"You are an expert in human activity recognition. Based on the following sensor data, \"\n",
    "        \"determine the most likely human activity being performed:\\n\"\n",
    "        f\"Mean acceleration in x-axis: {features['mean_x']:.2f} m/s^2\\n\"\n",
    "        f\"Mean acceleration in y-axis: {features['mean_y']:.2f} m/s^2\\n\"\n",
    "        f\"Mean acceleration in z-axis: {features['mean_z']:.2f} m/s^2\\n\"\n",
    "        f\"Standard deviation in x-axis: {features['std_x']:.2f} m/s^2\\n\"\n",
    "        f\"Standard deviation in y-axis: {features['std_y']:.2f} m/s^2\\n\"\n",
    "        f\"Standard deviation in z-axis: {features['std_z']:.2f} m/s^2\\n\"\n",
    "        \"Possible activities include walking, sitting, standing, laying, walking upstairs, \"\n",
    "        \"and walking downstairs. Please state the most likely activity.\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Classify the activity based on the features using Zero-Shot Learning\n",
    "def classify_activity(features):\n",
    "    prompt = create_prompt(features)\n",
    "    try:\n",
    "        # Use the LLM to generate a response\n",
    "        response = llm.generate(prompt)\n",
    "        print(f\"Generated Prompt:\\n{prompt}\")  # For debugging\n",
    "        print(f\"Full Model Response: {response}\")  # For debugging\n",
    "        # Extract and return the response text\n",
    "        prediction = response.strip().split(\"\\n\")[0] if response else \"Unknown\"\n",
    "        return prediction if prediction in activity_descriptions else \"Unknown\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model generation: {e}\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(data):\n",
    "    df = pd.DataFrame(data.reshape(-1, 3), columns=['accx', 'accy', 'accz'])\n",
    "    \n",
    "    features = {\n",
    "        'mean_x': df['accx'].mean(),\n",
    "        'mean_y': df['accy'].mean(),\n",
    "        'mean_z': df['accz'].mean(),\n",
    "        'std_x': df['accx'].std(),\n",
    "        'std_y': df['accy'].std(),\n",
    "        'std_z': df['accz'].std()\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def main():\n",
    "    # Randomly select 10 samples from X_train and corresponding y_train\n",
    "    num_samples = 10\n",
    "    random_indices = np.random.choice(X_train.shape[0], size=num_samples, replace=False)\n",
    "    \n",
    "    # Correctly indexing rows using .iloc[]\n",
    "    X_samples = X_train.iloc[random_indices]\n",
    "    y_true = y_train.iloc[random_indices]\n",
    "\n",
    "    predictions = []\n",
    "    for data in X_samples.values:  # .values returns the underlying numpy array\n",
    "        # Extract features from each sample\n",
    "        features = extract_features(data)\n",
    "        \n",
    "        # Classify activity\n",
    "        predicted_activity = classify_activity(features)\n",
    "        predictions.append(predicted_activity)\n",
    "    \n",
    "    # Convert predictions to a numpy array\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate and print accuracy\n",
    "    accuracy = np.mean(predictions == y_true.values) * 100\n",
    "    print(\"Predictions:\", predictions)\n",
    "    print(\"True labels:\", y_true.values) \n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Learning:\n",
      "0) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "1) Actual: WALKING, Predicted: WALKING\n",
      "2) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "3) Actual: STANDING, Predicted: WALKING\n",
      "4) Actual: STANDING, Predicted: WALKING\n",
      "5) Actual: WALKING, Predicted: WALKING_UPSTAIRS\n",
      "6) Actual: WALKING, Predicted: WALKING\n",
      "7) Actual: STANDING, Predicted: WALKING\n",
      "8) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "9) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "10) Actual: LAYING, Predicted: WALKING\n",
      "11) Actual: STANDING, Predicted: WALKING\n",
      "12) Actual: LAYING, Predicted: WALKING\n",
      "13) Actual: STANDING, Predicted: WALKING\n",
      "14) Actual: LAYING, Predicted: WALKING_UPSTAIRS\n",
      "15) Actual: WALKING, Predicted: WALKING\n",
      "16) Actual: LAYING, Predicted: WALKING\n",
      "17) Actual: STANDING, Predicted: WALKING\n",
      "18) Actual: WALKING_UPSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "19) Actual: STANDING, Predicted: WALKING\n",
      "20) Actual: SITTING, Predicted: WALKING\n",
      "21) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "22) Actual: WALKING_UPSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "23) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "24) Actual: WALKING, Predicted: WALKING\n",
      "25) Actual: SITTING, Predicted: WALKING\n",
      "26) Actual: LAYING, Predicted: WALKING\n",
      "27) Actual: SITTING, Predicted: WALKING\n",
      "28) Actual: WALKING, Predicted: WALKING\n",
      "29) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "30) Actual: LAYING, Predicted: WALKING\n",
      "31) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "32) Actual: SITTING, Predicted: WALKING\n",
      "33) Actual: SITTING, Predicted: WALKING\n",
      "34) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "35) Actual: LAYING, Predicted: WALKING\n",
      "36) Actual: LAYING, Predicted: WALKING\n",
      "37) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "38) Actual: WALKING, Predicted: WALKING\n",
      "39) Actual: STANDING, Predicted: WALKING\n",
      "40) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "41) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "42) Actual: WALKING, Predicted: WALKING\n",
      "43) Actual: SITTING, Predicted: WALKING\n",
      "44) Actual: SITTING, Predicted: WALKING\n",
      "45) Actual: SITTING, Predicted: WALKING_UPSTAIRS\n",
      "46) Actual: STANDING, Predicted: WALKING\n",
      "47) Actual: WALKING, Predicted: WALKING_UPSTAIRS\n",
      "48) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "49) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "50) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "51) Actual: LAYING, Predicted: WALKING\n",
      "52) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "53) Actual: SITTING, Predicted: WALKING\n",
      "\n",
      "Total: 54\n",
      "Correct: 9\n",
      "Accuracy: 0.16666666666666666\n",
      "\n",
      "Zero-Shot Learning:\n",
      "0) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "1) Actual: WALKING, Predicted: WALKING\n",
      "2) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "3) Actual: STANDING, Predicted: WALKING\n",
      "4) Actual: STANDING, Predicted: WALKING\n",
      "5) Actual: WALKING, Predicted: WALKING_UPSTAIRS\n",
      "6) Actual: WALKING, Predicted: WALKING\n",
      "7) Actual: STANDING, Predicted: WALKING\n",
      "8) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "9) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "10) Actual: LAYING, Predicted: WALKING\n",
      "11) Actual: STANDING, Predicted: WALKING\n",
      "12) Actual: LAYING, Predicted: WALKING\n",
      "13) Actual: STANDING, Predicted: WALKING\n",
      "14) Actual: LAYING, Predicted: WALKING_UPSTAIRS\n",
      "15) Actual: WALKING, Predicted: WALKING\n",
      "16) Actual: LAYING, Predicted: WALKING\n",
      "17) Actual: STANDING, Predicted: WALKING\n",
      "18) Actual: WALKING_UPSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "19) Actual: STANDING, Predicted: WALKING\n",
      "20) Actual: SITTING, Predicted: WALKING\n",
      "21) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "22) Actual: WALKING_UPSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "23) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "24) Actual: WALKING, Predicted: WALKING\n",
      "25) Actual: SITTING, Predicted: WALKING\n",
      "26) Actual: LAYING, Predicted: WALKING\n",
      "27) Actual: SITTING, Predicted: WALKING\n",
      "28) Actual: WALKING, Predicted: WALKING\n",
      "29) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "30) Actual: LAYING, Predicted: WALKING\n",
      "31) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "32) Actual: SITTING, Predicted: WALKING\n",
      "33) Actual: SITTING, Predicted: WALKING\n",
      "34) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "35) Actual: LAYING, Predicted: WALKING\n",
      "36) Actual: LAYING, Predicted: WALKING\n",
      "37) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "38) Actual: WALKING, Predicted: WALKING\n",
      "39) Actual: STANDING, Predicted: WALKING\n",
      "40) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "41) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "42) Actual: WALKING, Predicted: WALKING\n",
      "43) Actual: SITTING, Predicted: WALKING\n",
      "44) Actual: SITTING, Predicted: WALKING\n",
      "45) Actual: SITTING, Predicted: WALKING_UPSTAIRS\n",
      "46) Actual: STANDING, Predicted: WALKING\n",
      "47) Actual: WALKING, Predicted: WALKING_UPSTAIRS\n",
      "48) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "49) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "50) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "51) Actual: LAYING, Predicted: WALKING\n",
      "52) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "53) Actual: SITTING, Predicted: WALKING\n",
      "\n",
      "Total: 54\n",
      "Correct: 9\n",
      "Accuracy: 0.16666666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API key from the environment variables\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Available Models\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "model = groq_models[\"llama3-70b\"]\n",
    "\n",
    "# Constants\n",
    "time = 10\n",
    "offset = 100\n",
    "folders = [\"LAYING\", \"SITTING\", \"STANDING\", \"WALKING\", \"WALKING_DOWNSTAIRS\", \"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\": 1, \"WALKING_UPSTAIRS\": 2, \"WALKING_DOWNSTAIRS\": 3, \"SITTING\": 4, \"STANDING\": 5, \"LAYING\": 6}\n",
    "\n",
    "combined_dir = os.path.join(\"C:\\\\Users\\\\arpit\\\\OneDrive\\\\Desktop\\\\es335-24-fall-assignment-1\\\\UCI HAR Dataset\\\\Combined\")\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "dataset_dir = os.path.join(combined_dir, \"Train\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_train.append(df.values)\n",
    "        y_train.append(classes[folder])\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "dataset_dir = os.path.join(combined_dir, \"Test\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_test.append(df.values)\n",
    "        y_test.append(classes[folder])\n",
    "\n",
    "# Concatenate the training and testing data\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "seed = 4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "\n",
    "categories = list(classes.keys())\n",
    "\n",
    "# Define the ChatGroq LLM\n",
    "llm = ChatGroq(model=model, api_key=GROQ_API_KEY, temperature=0)\n",
    "\n",
    "# Few-Shot Learning Example\n",
    "print(\"Few-Shot Learning:\")\n",
    "few_shot_examples = 3  # Using only 3 examples per activity for training\n",
    "\n",
    "# Train prompt template\n",
    "train_prompt = lambda data, category: f\"\"\"\n",
    "You do not need to output anything for this query.\n",
    "Example data for \"{category}\":\n",
    "{data}\n",
    "\"\"\"\n",
    "\n",
    "# Test prompt template\n",
    "test_prompt = lambda data: f\"\"\"\n",
    "You will be provided 3-axis accelerometer data from one of the following activities:\n",
    "    1) WALKING\n",
    "    2) LAYING\n",
    "    3) WALKING_UPSTAIRS\n",
    "    4) WALKING_DOWNSTAIRS\n",
    "    5) SITTING\n",
    "    6) STANDING\n",
    "You have to output only the name of the predicted activity and nothing else.\n",
    "DATA:\n",
    "{data}\n",
    "\"\"\"\n",
    "\n",
    "# Training with Few-Shot Learning\n",
    "for i in range(few_shot_examples):\n",
    "    for j in range(len(folders)):\n",
    "        query = train_prompt(X_train[i + j * few_shot_examples], categories[y_train[i + j * few_shot_examples] - 1])\n",
    "        llm.invoke(query)\n",
    "\n",
    "# Testing the model\n",
    "correct_count = 0\n",
    "for i in range(len(X_test)):\n",
    "    query = test_prompt(X_test[i])\n",
    "    ans = llm.invoke(query).content\n",
    "    if ans == categories[y_test[i] - 1]:\n",
    "        correct_count += 1\n",
    "    print(f\"{i}) Actual: {categories[y_test[i] - 1]}, Predicted: {ans}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Total: {len(X_test)}\n",
    "Correct: {correct_count}\n",
    "Accuracy: {correct_count/len(X_test)}\n",
    "\"\"\")\n",
    "\n",
    "# Zero-Shot Learning Example\n",
    "print(\"Zero-Shot Learning:\")\n",
    "\n",
    "correct_count = 0\n",
    "for i in range(len(X_test)):\n",
    "    query = test_prompt(X_test[i])\n",
    "    ans = llm.invoke(query).content\n",
    "    if ans == categories[y_test[i] - 1]:\n",
    "        correct_count += 1\n",
    "    print(f\"{i}) Actual: {categories[y_test[i] - 1]}, Predicted: {ans}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Total: {len(X_test)}\n",
    "Correct: {correct_count}\n",
    "Accuracy: {correct_count/len(X_test)}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Learning:\n",
      "0) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "1) Actual: WALKING, Predicted: WALKING\n",
      "2) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "3) Actual: STANDING, Predicted: WALKING\n",
      "4) Actual: STANDING, Predicted: WALKING\n",
      "5) Actual: WALKING, Predicted: WALKING_UPSTAIRS\n",
      "6) Actual: WALKING, Predicted: WALKING\n",
      "7) Actual: STANDING, Predicted: WALKING\n",
      "8) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "9) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "10) Actual: LAYING, Predicted: WALKING\n",
      "11) Actual: STANDING, Predicted: WALKING\n",
      "12) Actual: LAYING, Predicted: WALKING\n",
      "13) Actual: STANDING, Predicted: WALKING\n",
      "14) Actual: LAYING, Predicted: WALKING_UPSTAIRS\n",
      "15) Actual: WALKING, Predicted: WALKING\n",
      "16) Actual: LAYING, Predicted: WALKING\n",
      "17) Actual: STANDING, Predicted: WALKING\n",
      "18) Actual: WALKING_UPSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "19) Actual: STANDING, Predicted: WALKING\n",
      "20) Actual: SITTING, Predicted: WALKING\n",
      "21) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "22) Actual: WALKING_UPSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "23) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "24) Actual: WALKING, Predicted: WALKING\n",
      "25) Actual: SITTING, Predicted: WALKING\n",
      "26) Actual: LAYING, Predicted: WALKING\n",
      "27) Actual: SITTING, Predicted: WALKING\n",
      "28) Actual: WALKING, Predicted: WALKING\n",
      "29) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "30) Actual: LAYING, Predicted: WALKING\n",
      "31) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "32) Actual: SITTING, Predicted: WALKING\n",
      "33) Actual: SITTING, Predicted: WALKING\n",
      "34) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "35) Actual: LAYING, Predicted: WALKING\n",
      "36) Actual: LAYING, Predicted: WALKING\n",
      "37) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "38) Actual: WALKING, Predicted: WALKING\n",
      "39) Actual: STANDING, Predicted: WALKING\n",
      "40) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "41) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "42) Actual: WALKING, Predicted: WALKING\n",
      "43) Actual: SITTING, Predicted: WALKING\n",
      "44) Actual: SITTING, Predicted: WALKING\n",
      "45) Actual: SITTING, Predicted: WALKING_UPSTAIRS\n",
      "46) Actual: STANDING, Predicted: WALKING\n",
      "47) Actual: WALKING, Predicted: WALKING_UPSTAIRS\n",
      "48) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "49) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "50) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING_UPSTAIRS\n",
      "51) Actual: LAYING, Predicted: WALKING\n",
      "52) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "53) Actual: SITTING, Predicted: WALKING\n",
      "Zero-Shot Learning:\n",
      "0) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "1) Actual: WALKING, Predicted: WALKING\n",
      "2) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "3) Actual: STANDING, Predicted: WALKING\n",
      "4) Actual: STANDING, Predicted: WALKING\n",
      "5) Actual: WALKING, Predicted: WALKING_UPSTAIRS\n",
      "6) Actual: WALKING, Predicted: WALKING\n",
      "7) Actual: STANDING, Predicted: WALKING\n",
      "8) Actual: WALKING_DOWNSTAIRS, Predicted: WALKING\n",
      "9) Actual: WALKING_UPSTAIRS, Predicted: WALKING\n",
      "Zero-Shot Learning Accuracy: 0.2\n",
      "Few-Shot Learning Accuracy: 0.16666666666666666\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API key from the environment variables\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Available Models\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "model = groq_models[\"llama3-70b\"]\n",
    "\n",
    "# Constants\n",
    "time = 10\n",
    "offset = 100\n",
    "folders = [\"LAYING\", \"SITTING\", \"STANDING\", \"WALKING\", \"WALKING_DOWNSTAIRS\", \"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\": 1, \"WALKING_UPSTAIRS\": 2, \"WALKING_DOWNSTAIRS\": 3, \"SITTING\": 4, \"STANDING\": 5, \"LAYING\": 6}\n",
    "\n",
    "combined_dir = os.path.join(\"C:\\\\Users\\\\arpit\\\\OneDrive\\\\Desktop\\\\es335-24-fall-assignment-1\\\\UCI HAR Dataset\\\\Combined\")\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "dataset_dir = os.path.join(combined_dir, \"Train\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_train.append(df.values)\n",
    "        y_train.append(classes[folder])\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "dataset_dir = os.path.join(combined_dir, \"Test\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_test.append(df.values)\n",
    "        y_test.append(classes[folder])\n",
    "\n",
    "# Concatenate the training and testing data\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "seed = 4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "\n",
    "categories = list(classes.keys())\n",
    "\n",
    "# Define the ChatGroq LLM\n",
    "llm = ChatGroq(model=model, api_key=GROQ_API_KEY, temperature=0)\n",
    "\n",
    "# Few-Shot Learning Example (5 examples per class)\n",
    "print(\"Few-Shot Learning:\")\n",
    "few_shot_examples_per_class = 5  # Number of examples per class\n",
    "\n",
    "# Train prompt template\n",
    "train_prompt = lambda data, category: f\"\"\"\n",
    "You do not need to output anything for this query.\n",
    "Example data for \"{category}\":\n",
    "{data}\n",
    "\"\"\"\n",
    "\n",
    "# Test prompt template\n",
    "test_prompt = lambda data: f\"\"\"\n",
    "You will be provided 3-axis accelerometer data from one of the following activities:\n",
    "    1) WALKING\n",
    "    2) LAYING\n",
    "    3) WALKING_UPSTAIRS\n",
    "    4) WALKING_DOWNSTAIRS\n",
    "    5) SITTING\n",
    "    6) STANDING\n",
    "You have to output only the name of the predicted activity and nothing else.\n",
    "DATA:\n",
    "{data}\n",
    "\"\"\"\n",
    "\n",
    "# Training with Few-Shot Learning (5 examples per class)\n",
    "used_indices = set()  # To track used examples and avoid duplicates\n",
    "for folder in folders:\n",
    "    class_indices = [i for i, label in enumerate(y_train) if label == classes[folder]]\n",
    "    selected_indices = class_indices[:few_shot_examples_per_class]  # Select the first 5 examples for this class\n",
    "\n",
    "    for index in selected_indices:\n",
    "        if index not in used_indices:\n",
    "            query = train_prompt(X_train[index], categories[y_train[index] - 1])\n",
    "            llm.invoke(query)\n",
    "            used_indices.add(index)  # Mark this example as used\n",
    "\n",
    "# Testing the model (Few-Shot Learning)\n",
    "correct_count_fs = 0\n",
    "for i in range(len(X_test)):\n",
    "    query = test_prompt(X_test[i])\n",
    "    ans = llm.invoke(query).content\n",
    "    if ans == categories[y_test[i] - 1]:\n",
    "        correct_count_fs += 1\n",
    "    print(f\"{i}) Actual: {categories[y_test[i] - 1]}, Predicted: {ans}\")\n",
    "\n",
    "\n",
    "\n",
    "# Zero-Shot Learning Example (10 examples)\n",
    "print(\"Zero-Shot Learning:\")\n",
    "zero_shot_examples = 10  # Using only 10 examples for testing\n",
    "\n",
    "correct_count_zs = 0\n",
    "for i in range(zero_shot_examples):\n",
    "    query = test_prompt(X_test[i])\n",
    "    ans = llm.invoke(query).content\n",
    "    if ans == categories[y_test[i] - 1]:\n",
    "        correct_count_zs += 1\n",
    "    print(f\"{i}) Actual: {categories[y_test[i] - 1]}, Predicted: {ans}\")\n",
    "\n",
    "accuracy_zs = correct_count_zs / zero_shot_examples\n",
    "print(f\"Zero-Shot Learning Accuracy: {accuracy_zs}\")\n",
    "accuracy_fs = correct_count_fs / len(X_test)\n",
    "print(f\"Few-Shot Learning Accuracy: {accuracy_fs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
